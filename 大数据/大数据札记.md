# 大数据 学习札记

- [大数据 学习札记](#大数据-学习札记)
    - [参考资料](#参考资料)
    - [hadoop](#hadoop)
        - [参考](#参考)
        - [概念](#概念)
        - [使用](#使用)
    - [hue](#hue)
    - [hive](#hive)
    - [kylin](#kylin)
    - [presto](#presto)
        - [安装](#安装)
        - [presto 连接hive](#presto-连接hive)
        - [presto webui - yanagishima](#presto-webui---yanagishima)
    - [sqoop](#sqoop)
    - [Ambari](#ambari)

## 参考资料

[Hadoop教程&trade;](https://www.yiibai.com/hadoop/)

[MapReduce(一)  mapreduce基础入门 - 凌-风 - 博客园](https://www.cnblogs.com/liuwei6/p/6708116.html)

[presto、druid、sparkSQL、kylin的对比分析，如性能、架构等，有什么异同？ - 知乎](https://www.zhihu.com/question/41541395?sort=created)

[环境搭建 Hadoop+Hive(orcfile格式)+Presto实现大数据存储查询一 - 翟中龙 - 博客园](https://www.cnblogs.com/nyzhai/p/6102423.html)

## hadoop

### 参考

[Hadoop_常用存储与压缩格式](https://www.cnblogs.com/eRrsr/p/6098454.html)
[hadoop 四种压缩格式](https://blog.csdn.net/lifuxiangcaohui/article/details/52623660)

### 概念

>hadoop 的四大组件：
HDFS：分布式存储系统
MapReduce：分布式计算系统
YARN： hadoop 的资源调度系统
Common： 以上三大组件的底层支撑组件，主要提供基础工具包和 RPC 框架等

>Namenode：是整个文件系统的管理节点。它维护着1.整个文件系统的文件目录树，2.文件/目录的元信息和每个文件对应的数据块列表。3.接收用户的操作请求。
Datanode：提供真实文件数据的存储服务。

>以Apache Hadoop为主导的大数据技术的出现，使得中小型公司对于大数据的存储与处理也拥有了武器。目前Hadoop有不少发行版：华为发行版 收费 、Intel发行版 收费 、Cloudera发行版（Cloudera’s Distribution Including Apache Hadoop，简称 CDH ） 免费 、Hortonworks发行版（Hortonworks Data Platform，简称 HDP ） 免费 等，所有这些发行版均是基于Apache Hadoop社区版衍生出来的。

### 使用

操作hdfs系统可以使用hadoop fs 也可以使用 hdfs dfs ,两者效果一样。(hadoop dfs命令已不再建议使用)

[hadoop中的hadoop和hdfs和yarn命令](https://www.cnblogs.com/lzfhope/p/6952869.html)
[Hadoop Hdfs常用命令](https://blog.csdn.net/sunshingheavy/article/details/53227581)

```bash
# [HDFS基本命令的使用](https://blog.csdn.net/afafawfaf/article/details/80254989)
hadoop fs -ls  显示当前目录结构，-ls -R 递归显示目录结构
hadoop fs -mkdir  创建目录
hadoop fs -rm   删除文件，-rm -R 递归删除目录和文件
hadoop fs -put  [localsrc] [dst]  从本地加载文件到HDFS
hadoop fs -get  [dst] [localsrc]  从HDFS导出文件到本地
hadoop fs -copyFromLocal [localsrc] [dst]  从本地加载文件到HDFS，与put一致
hadoop fs -copyToLocal [dst] [localsrc]  从HDFS导出文件到本地，与get一致
hadoop fs -test -e  检测目录和文件是否存在，存在返回值$?为0，不存在返回1
hadoop fs -text  查看文件内容
hadoop fs -du  统计目录下各文件大小，单位字节。-du -s 汇总目录下文件大小，-du -h 显示单位
hadoop fs -tail  显示文件末尾
hadoop fs -cp [src] [dst] 从源目录复制文件到目标目录
hadoop fs -mv [src] [dst] 从源目录移动文件到目标目录
```

## hue

---

## hive

[Hive快速入门 - CSDN博客](https://blog.csdn.net/trigl/article/details/61418571)

---

## kylin

[Kylin的cube模型 - Treant - 博客园](https://www.cnblogs.com/en-heng/p/5239311.html)
[Kylin介绍 （很有用） - 一片黑 - 博客园](https://www.cnblogs.com/honey01/p/8351145.html)
[Kylin的Hierarchies，Derived维度方面配置优化 - CSDN博客](https://blog.csdn.net/jiangshouzhuang/article/details/51286150)

---

## presto

>实际业务中用Presto的原因
>
>1. 第一点也是最重要的一点跨库和跨数据源，什么概念？跨库：mysql中有A库a表，B库b表，用Presto SQL可以实现a,b两表的join操作。跨数据源:mysql中有A库a表，oracle中有B库b表，用Presto SQL可以实现a,b两表的join操作。
>2. Presto SQL和传统SQL大同小异，基本对于熟悉SQL的人员0成本进入
>3. 数据量支持GB到PB字节
>4. 集群部署，可水平扩展增强大的Presto SQL查询速度
>5. 客户端可以是PHP，JAVA, Python, 命令行等等，这样可以作为一个平台级的中间件给团队提供支撑
>6. Presto是JAVA书写而成，我是JAVA出身，对报错或者配置更熟悉
>
>EN官网:https://prestodb.io/
CN官网:http://prestodb-china.com/
GitHub:https://github.com/prestodb/presto

### 安装

Ambari安装presto
[Ambari安装Presto(离线)](https://blog.csdn.net/wangpei1949/article/details/79952539)
[ambari-presto-service 官方文档](https://prestodb.io/ambari-presto-service/)
[github/ambari-presto-service](https://github.com/prestodb/ambari-presto-service/releases)

[Ambari界面Presto重启报错ERROR:presto_client:Error connecting to presto server at: localhost:8285](https://blog.csdn.net/ZhouyuanLinli/article/details/77970542)

### presto 连接hive

[4.2. Hive连接器 &mdash; Presto 0.100 Documentation](http://prestodb-china.com/docs/current/connector/hive.html)

### presto webui - yanagishima

yanagishima可以连接hive和presto，搭建简单，功能也可以满足基本需要，github也一直在维护，推荐。
https://github.com/yanagishima/yanagishima

---

## sqoop

[[总结]----Sqoop 几种导入导出模式](https://blog.csdn.net/james__tao/article/details/78106396)
[使用Sqoop从MySQL导入数据到Hive和HBase](https://www.cnblogs.com/wgp13x/p/5028220.html)
[学习Hadoop第二十二课（Sqoop---数据迁移工具）](https://blog.csdn.net/anaitudou/article/details/80931994)
[Sqoop 加载数据到ORC表 - CSDN博客](https://blog.csdn.net/Post_Yuan/article/details/64444724)
[Sqoop将数据导入到hive时，数据错位的问题解决](https://www.2cto.com/database/201803/733896.html)
[Sqoop import as OrC file](https://stackoverflow.com/questions/29978522/sqoop-import-as-orc-file)
[sqoop指定多个字段导入类型 - CSDN博客](https://blog.csdn.net/wisdom_c_1010/article/details/78841196)

```bash
sqoop import --connect jdbc:mysql://localhost:3306/sqoop --username root --password root --table Student --direct
# 导入mysql表添加 --direct 参数速度更快，执行的是mysq自带的导出功能


#压缩
sqoop import \
-m 1 \
--connect jdbc:mysql://localhost:3306/test?useSSL=false --password root --username root \
--table dw_slice_rule --hive-table dw_slice_rule_compress --delete-target-dir --direct \
--hive-import --hive-database default --hive-overwrite --create-hive-table --delete-target-dir \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec

# 非压缩
sqoop import -m 1 --connect jdbc:mysql://localhost:3306/test?useSSL=false --password root --username root --table dw_slice_rule --hive-table dw_slice_rule --hive-import --hive-database default --hive-overwrite --create-hive-table  --delete-target-dir --direct
```

**orc导入** (使用hcatalog方式导入 不存在数据错位问题)

```bash
sqoop import \
-m 1 \
--connect jdbc:mysql://localhost:3306/test?useSSL=false --password root --username root \
--create-hcatalog-table --hcatalog-database my_hcat_db \
--table dw_slice_rule --hcatalog-table dw_slice_rule_orc \
--hcatalog-storage-stanza 'stored as orc tblproperties ("orc.compress"="SNAPPY")' --delete-target-dir

sqoop import \
 --connect jdbc:postgresql://foobar:5432/my_db \
 --driver org.postgresql.Driver \
 --connection-manager org.apache.sqoop.manager.GenericJdbcManager \
 --username foo \
 --password-file hdfs:///user/foobar/foo.txt \
 --table fact \
 --hcatalog-home /usr/hdp/current/hive-webhcat \
 --hcatalog-database my_hcat_db \
 --hcatalog-table fact \
 --create-hcatalog-table \
 --hcatalog-storage-stanza 'stored as orc tblproperties ("orc.compress"="SNAPPY")'
```

where条件导入

```bash
sqoop import \
--connect jdbc:mysql://localhost:9800/boss?useSSL=false --password root --username root \
--table crm_account --hive-table crm_account_dimension \
--hive-drop-import-delims \
--delete-target-dir --num-mappers 1 \
--hive-import --hive-database dc_measure --hive-overwrite --create-hive-table \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec

# --hive-drop-import-delims 用于解决数据导入时数据错位以及产生null值问题 [sqoop 从oracle到hive数据错位，并产生很多null值](https://blog.csdn.net/duyuanhai/article/details/76021549)
```

增量导入
[Sqoop增量导入实战 - CSDN博客](https://blog.csdn.net/qq_26937525/article/details/53670213)

```bash
#!/bin/bash
month=(201704 201705 201706 201707 201708 201709 201710 201711 201712)

for i in ${month[@]}
do

echo ">>>>import: $i"

sqoop import \
--connect jdbc:mysql://localhost:9901/mydb?useSSL=false --password 123456 --username root \
--incremental append \
--check-column stat_date \
--last-value "2017-04-31" \
--table dw_result_product_bill_detail_$i --hive-table dw_result_product_bill_detail_merge \
--hive-drop-import-delims \
--hive-import --hive-database bill \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec

done
```

---

## Ambari

hadoop集群可视化部署工具，具备Hadoop组件的安装、管理、运维等基本功能，提供Web UI进行可视化的集群管理，简化了大数据平台的安装、使用难度。
[Ambari 功能简介](https://blog.csdn.net/u012331758/article/details/78106714)
